{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"/workspace/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string, random\n",
    "from collections import Counter \n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "\n",
    "part_user = set()\n",
    "    # Users participated in Asking and Answering\n",
    "\n",
    "# count how many questions an users asked\n",
    "# count how many questions an answerer responded\n",
    "count_Q, count_A = {}, {}\n",
    "\n",
    "qa_map = {}\n",
    "test_candidates = set()\n",
    "# \n",
    "def clean_html(x):\n",
    "    return BeautifulSoup(x, 'lxml').get_text()\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"Clean up the string\n",
    "\n",
    "    Cleaning strings of content or title\n",
    "    Original taken from [https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py]\n",
    "\n",
    "    Args:\n",
    "        string - the string to clean\n",
    "\n",
    "    Return:\n",
    "        _ - the cleaned string\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def clean_str2(s):\n",
    "    \"\"\"Clean up the string\n",
    "\n",
    "    * New version, removing all punctuations\n",
    "\n",
    "    Cleaning strings of content or title\n",
    "    Original taken from [https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py]\n",
    "\n",
    "    Args:\n",
    "        string - the string to clean\n",
    "\n",
    "    Return:\n",
    "        _ - the cleaned string\n",
    "    \"\"\"\n",
    "    ss = s\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    ss = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", ss)\n",
    "    ss = re.sub(r\"\\'s\", \"s\", ss)\n",
    "    ss = re.sub(r\"\\'ve\", \"ve\", ss)\n",
    "    ss = re.sub(r\"n\\'t\", \"nt\", ss)\n",
    "    ss = re.sub(r\"\\'re\", \"re\", ss)\n",
    "    ss = re.sub(r\"\\'d\", \"d\", ss)\n",
    "    ss = re.sub(r\"\\'ll\", \"ll\", ss)\n",
    "    ss = re.sub(r\"\\s{2,}\", \" \", ss)\n",
    "    ss = ss.translate(translator)\n",
    "    return ss.strip().lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(string, stopword_set):\n",
    "    \"\"\"Removing Stopwords\n",
    "\n",
    "    Args:\n",
    "        string - the input string to remove stopwords\n",
    "        stopword_set - the set of stopwords\n",
    "\n",
    "    Return:\n",
    "        _ - the string that has all the stopwords removed\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_string = [word for word in word_tokens\n",
    "                       if word not in stopword_set]\n",
    "    return \" \".join(filtered_string)\n",
    "\n",
    "\n",
    "def split_post(raw_dir, data_dir):\n",
    "    \"\"\" Split the post\n",
    "\n",
    "    Split post to question and answer,\n",
    "    keep all information, output to file\n",
    "\n",
    "    Args:\n",
    "        raw_dir - raw data directory\n",
    "        data_dir - parsed data directory\n",
    "    \"\"\"\n",
    "    if os.path.exists(data_dir + \"Posts_Q.json\") \\\n",
    "        and os.path.exists(data_dir + \"Posts_A.json\"):\n",
    "        print(\"\\t\\tPosts_Q.json, Posts_A.json already exists.\"\n",
    "              \"Skipping the split_post.\")\n",
    "        return\n",
    "\n",
    "    with open(data_dir + \"Posts_Q.json\", \"w\") as fout_q, \\\n",
    "            open(data_dir + \"Posts_A.json\", \"w\") as fout_a:\n",
    "        parser = etree.iterparse(raw_dir + 'Posts.xml',\n",
    "                                 events=('end',), tag='row')\n",
    "        for event, elem in parser:\n",
    "            attr = dict(elem.attrib)\n",
    "            attr['Body'] = clean_html(attr['Body'])\n",
    "\n",
    "            # Output to separate files\n",
    "            if attr['PostTypeId'] == '1':\n",
    "                fout_q.write(json.dumps(attr) + \"\\n\")\n",
    "            elif attr['PostTypeId'] == '2':\n",
    "                fout_a.write(json.dumps(attr) + \"\\n\")\n",
    "    return\n",
    "\n",
    "\n",
    "def process_QA(data_dir):\n",
    "    \"\"\"Process QA\n",
    "\n",
    "    Extract attributes used in this project\n",
    "    Get rid of the text information,\n",
    "    only record the question-user - answer-user relation\n",
    "\n",
    "    Args:\n",
    "        data_dir - the dir where primitive data is stored\n",
    "    \"\"\"\n",
    "    POST_Q = \"Posts_Q.json\"\n",
    "    POST_A = \"Posts_A.json\"\n",
    "    OUTPUT = \"Record_All.json\"\n",
    "    RAW_STATS = \"question.stats.raw\"\n",
    "\n",
    "    # Get logger to log exceptions\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    no_acc_question = 0\n",
    "\n",
    "    raw_question_stats = []\n",
    "\n",
    "    if not os.path.exists(data_dir + POST_Q):\n",
    "        raise IOError(\"file {} does NOT exist\".format(data_dir + POST_Q))\n",
    "\n",
    "    if not os.path.exists(data_dir + POST_A):\n",
    "        raise IOError(\"file {} does NOT exist\".format(data_dir + POST_A))\n",
    "\n",
    "    # Process question information\n",
    "    with open(data_dir + POST_Q, 'r') as fin_q:\n",
    "        for line in fin_q:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                qid, rid = data.get('Id', None), data.get('OwnerUserId', None)\n",
    "                # If such \n",
    "                if qid and rid:\n",
    "                    acc_id = data.get('AcceptedAnswerId', None)\n",
    "                    answer_count = int(data.get('AnswerCount', -1))\n",
    "                    if acc_id:\n",
    "                        qa_map[qid] = {\n",
    "                            'QuestionId': qid,\n",
    "                            'QuestionOwnerId': rid,\n",
    "                            'AcceptedAnswerId': acc_id,\n",
    "                            'AcceptedAnswererId': None,\n",
    "                            'AnswererIdList': [],\n",
    "                            'AnswererAnswerTuples': []\n",
    "                        }\n",
    "                        count_Q[rid] = count_Q.get(rid, 0) + 1\n",
    "                    else:\n",
    "                        no_acc_question += 1\n",
    "\n",
    "                    if answer_count >= 0:\n",
    "                        raw_question_stats.append(answer_count)\n",
    "            except:\n",
    "                logger.error(\"Error at process_QA 1: \" + str(data))\n",
    "                continue\n",
    "    print(\"\\t\\t{} questions do not have accepted answer!\"\n",
    "          .format(no_acc_question))\n",
    "    \n",
    "    # Count raw question statistics\n",
    "    raw_question_stats_cntr = Counter(raw_question_stats)\n",
    "    with open(data_dir + RAW_STATS, \"w\") as fout:\n",
    "        for x in sorted(list(raw_question_stats_cntr.keys())):\n",
    "            print(\"{}\\t{}\".format(x, raw_question_stats_cntr[x]), file=fout)\n",
    "        print(\"Total\\t{}\".format(sum(raw_question_stats)), file=fout)\n",
    "\n",
    "    # Process answer information\n",
    "    with open(data_dir + POST_A, 'r') as fin_a:\n",
    "        for line in fin_a:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                answer_id = data.get('Id', None)\n",
    "                aid = data.get('OwnerUserId', None)\n",
    "                qid = data.get('ParentId', None)\n",
    "                score = data.get('Score', None)\n",
    "                entry = qa_map.get(qid, None)\n",
    "                if answer_id and aid and qid and entry and score:\n",
    "                    entry['AnswererAnswerTuples'].append((aid, answer_id,  score      ))\n",
    "                    entry['AnswererIdList'].append(aid)\n",
    "                    count_A[aid] = count_A.get(aid, 0) + 1\n",
    "\n",
    "                    # Check if we happen to hit the accepted answer\n",
    "                    if answer_id == entry['AcceptedAnswerId']:\n",
    "                        entry['AcceptedAnswererId'] = aid\n",
    "                else:\n",
    "                    logger.error(\n",
    "                        \"Answer {} belongs to unknown Question {} at Process QA\"\n",
    "                        .format(answer_id, qid))\n",
    "            except IndexError as e:\n",
    "                logger.error(e)\n",
    "                logger.info(\"Error at process_QA 2: \" + str(data))\n",
    "                continue\n",
    "\n",
    "    # Fill in the blanks of `AcceptedAnswererId`\n",
    "        for qid in qa_map.keys():\n",
    "            acc_id = qa_map[qid]['AcceptedAnswerId']\n",
    "            for aid, answer_id, score in qa_map[qid]['AnswererAnswerTuples']:\n",
    "                if answer_id == acc_id:\n",
    "                    qa_map[qid]['AcceptedAnswererId'] = aid\n",
    "                    break\n",
    "\n",
    "    print(\"\\t\\tWriting the Record for ALL to disk.\")\n",
    "    \n",
    "    with open(data_dir + OUTPUT, 'w') as fout:\n",
    "        for q in qa_map.keys():\n",
    "            fout.write(json.dumps(qa_map[q]) + \"\\n\")\n",
    "\n",
    "\n",
    "def question_stats(data_dir):\n",
    "    \"\"\"Find the question statistics for `Introduction`\n",
    "\n",
    "    Args:\n",
    "        data_dir -\n",
    "    Return\n",
    "    \"\"\"\n",
    "    OUTPUT = \"question.stats\"\n",
    "    count = []\n",
    "    for qid in qa_map.keys():\n",
    "        ans_count = len(qa_map[qid]['AnswererIdList'])\n",
    "        count.append(ans_count)\n",
    "        if ans_count == 0:\n",
    "            print(\"0 answer id list\", qid)\n",
    "    question_stats_cntr = Counter(count)\n",
    "\n",
    "    with open(data_dir + OUTPUT, \"w\") as fout:\n",
    "        for x in sorted(list(question_stats_cntr.keys())):\n",
    "            print(\"{}\\t{}\".format(x, question_stats_cntr[x]), file=fout)\n",
    "        print(\"Total\\t{}\".format(sum(count), file=fout), file=fout)\n",
    "    return\n",
    "\n",
    "\n",
    "def build_test_set(data_dir, parsed_dir, threshold, test_sample_size,\n",
    "                   test_proportion):\n",
    "    \"\"\"\n",
    "    Building test datase,\n",
    "    test_proportiont\n",
    "    Args:\n",
    "        parse_dir - the directory to save parsed set.\n",
    "        threshold - the selection threshold\n",
    "\n",
    "    Return:\n",
    "    \"\"\"\n",
    "    TEST = \"test.txt\"\n",
    "    OUTPUT_TRAIN = \"Record_Train.json\"\n",
    "    TRAINTEST=\"traintest.txt\"\n",
    "\n",
    "    accept_no_answerer = 0\n",
    "\n",
    "    ordered_count_A = sorted(\n",
    "        count_A.items(), key=lambda x:x[1], reverse=True)\n",
    "    ordered_aid = [x[0] for x in ordered_count_A]\n",
    "    ordered_aid = ordered_aid[: int(len(ordered_aid) * 0.14)]\n",
    "\n",
    "    question_count = len(qa_map)\n",
    "\n",
    "    for qid in qa_map.keys():\n",
    "        accaid = qa_map[qid]['AcceptedAnswererId']\n",
    "        rid = qa_map[qid]['QuestionOwnerId']\n",
    "        if not accaid:\n",
    "            accept_no_answerer += 1\n",
    "            continue\n",
    "        if count_Q[rid] >= threshold and count_A[accaid] >= threshold:\n",
    "            test_candidates.add(qid)\n",
    "\n",
    "    print(\"\\t\\tSample table size {}. Using {} instances for test.\"\n",
    "          .format(len(test_candidates), int(question_count * test_proportion)))\n",
    "\n",
    "    test = np.random.choice(list(test_candidates),\n",
    "                            size=int(question_count * test_proportion),\n",
    "                            replace=False)\n",
    "\n",
    "    print(\"\\t\\tAccepted answer without Answerer {}\".format(accept_no_answerer))\n",
    "\n",
    "    print(\"\\t\\tWriting the sampled test set to disk\")\n",
    "    with open(parsed_dir + TEST, \"w\") as fout:\n",
    "        \n",
    "        for qid in test:\n",
    "            #print(\"qid\",qid)\n",
    "            rid = qa_map[qid]['QuestionOwnerId']\n",
    "            accaid = qa_map[qid]['AcceptedAnswererId']\n",
    "            aid_list = qa_map[qid]['AnswererIdList']\n",
    "            if len(aid_list) <= test_sample_size:\n",
    "                #if len(aid_list)>20:\n",
    "                    #print(1)\n",
    "                neg_sample_size = test_sample_size - len(aid_list)  #test_sample_size - len(aid_list)\n",
    "                neg_samples = random.sample(ordered_aid, neg_sample_size)\n",
    "                samples = neg_samples +  aid_list\n",
    "                #print(len(samples),samples)\n",
    "            else:\n",
    "                samples = random.sample(aid_list, test_sample_size)\n",
    "            if accaid not in samples:\n",
    "                samples.pop()\n",
    "                samples.append(accaid)\n",
    "            samples = \" \".join(samples)\n",
    "\n",
    "            print(\"{} {} {} {}\".format(rid, qid, accaid, samples),\n",
    "                  file=fout)\n",
    "\n",
    "    # if qid is a test instance or qid doesn't have an answer\n",
    "    qid_list = list(qa_map.keys())\n",
    "    for qid in qid_list:\n",
    "        if qid in test\\\n",
    "            or not len(qa_map[qid]['AnswererIdList'])\\\n",
    "            or not qa_map[qid]['AcceptedAnswererId']:\n",
    "            del qa_map[qid]\n",
    "\n",
    "\n",
    "             \n",
    "            \n",
    "            \n",
    "    test_candidates1=set()       \n",
    "    for qid in qa_map.keys():\n",
    "        accaid = qa_map[qid]['AcceptedAnswererId']\n",
    "        rid = qa_map[qid]['QuestionOwnerId']\n",
    "        if not accaid:\n",
    "            accept_no_answerer += 1\n",
    "            continue\n",
    "        if count_Q[rid] >= threshold and count_A[accaid] >= threshold:\n",
    "            test_candidates1.add(qid)\n",
    "\n",
    "    print(\"\\t\\tSample table size {}. Using {} instances for test.\"\n",
    "          .format(len(test_candidates1), int(question_count * test_proportion)))\n",
    "\n",
    "    traintest = np.random.choice(list(test_candidates1),\n",
    "                            size=int(question_count * test_proportion),\n",
    "                            replace=False)\n",
    "\n",
    "    print(\"\\t\\tAccepted answer without Answerer {}\".format(accept_no_answerer))\n",
    "\n",
    "    print(\"\\t\\tWriting the sampled traintest set to disk\")\n",
    "    with open(parsed_dir + TRAINTEST, \"w\") as fout:\n",
    "        \n",
    "        for qid in traintest:\n",
    "            #print(\"1qid\",qid)\n",
    "            rid = qa_map[qid]['QuestionOwnerId']\n",
    "            accaid = qa_map[qid]['AcceptedAnswererId']\n",
    "            aid_list = qa_map[qid]['AnswererIdList']\n",
    "            if len(aid_list) <= test_sample_size:\n",
    "                #if len(aid_list)>20:\n",
    "                    #print(1)\n",
    "                neg_sample_size = test_sample_size - len(aid_list)  #test_sample_size - len(aid_list)\n",
    "                neg_samples = random.sample(ordered_aid, neg_sample_size)\n",
    "                samples = neg_samples +  aid_list\n",
    "                #print(len(samples),samples)\n",
    "            else:\n",
    "                samples = random.sample(aid_list, test_sample_size)\n",
    "            if accaid not in samples:\n",
    "                samples.pop()\n",
    "                samples.append(accaid)\n",
    "            samples = \" \".join(samples)\n",
    "\n",
    "            print(\"{} {} {} {}\".format(rid, qid, accaid, samples),\n",
    "                  file=fout)\n",
    "            \n",
    "            \n",
    "            \n",
    "      \n",
    "            \n",
    "            \n",
    "            \n",
    "    # Write QA pair to file\n",
    "    print(\"\\t\\tWriting the Record for training to disk\")\n",
    "    with open(data_dir + OUTPUT_TRAIN, 'w') as fout:\n",
    "        for q in qa_map.keys():\n",
    "            fout.write(json.dumps(qa_map[q]) + \"\\n\")\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_question_user(data_dir, parsed_dir):\n",
    "    \"\"\"Extract Question User pairs and output to file.\n",
    "    Extract \"Q\" and \"R\". Format:\n",
    "        <Qid> <Rid>\n",
    "    E.g.\n",
    "        101 40\n",
    "        145 351\n",
    "\n",
    "    Args:\n",
    "        data_dir - data directory\n",
    "        parsed_dir - parsed file directory\n",
    "    \"\"\"\n",
    "    INPUT = \"Record_Train.json\"\n",
    "    #INPUT = \"Record_All.json\"\n",
    "    OUTPUT = \"Q_R.txt\"\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT):\n",
    "        IOError(\"Can NOT find {}\".format(data_dir + INPUT))\n",
    "\n",
    "    with open(data_dir + INPUT, \"r\") as fin:\n",
    "        with open(parsed_dir + OUTPUT, \"w\") as fout:\n",
    "            for line in fin:\n",
    "                data = json.loads(line)\n",
    "                qid = data['QuestionId']\n",
    "                rid = data['QuestionOwnerId']\n",
    "                part_user.add(int(rid))  # Adding participated questioners\n",
    "                print(\"{} {}\".format(str(qid), str(rid)), file=fout)\n",
    "\n",
    "\n",
    "def extract_question_answer_user(data_dir, parsed_dir):\n",
    "    \"\"\"Extract Question, Answer User pairs and output to file.\n",
    "\n",
    "    (1) Extract \"Q\" - \"A\"\n",
    "        The list of AnswerOwnerList contains <aid>-<owner_id> pairs\n",
    "        Format:\n",
    "            <Qid> <Aid>\n",
    "        E.g.\n",
    "            100 1011\n",
    "            21 490\n",
    "\n",
    "    (2) Extract \"Q\" - Accepted answerer\n",
    "        Format:\n",
    "            <Qid> <Acc_Aid>\n",
    "    Args:\n",
    "        data_dir - data directory\n",
    "        parsed_dir - parsed file directory\n",
    "    \"\"\"\n",
    "    INPUT = \"Record_Train.json\"\n",
    "    OUTPUT_A = \"Q_A.txt\"\n",
    "    OUTPUT_ACC = \"Q_ACC.txt\"\n",
    "    OUTPUT_Ans=\"Q_Ans.txt\"\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT):\n",
    "        IOError(\"Can NOT find {}\".format(data_dir + INPUT))\n",
    "\n",
    "    with open(data_dir + INPUT, \"r\") as fin, \\\n",
    "            open(parsed_dir + OUTPUT_A, \"w\") as fout_a, \\\n",
    "            open(parsed_dir + OUTPUT_ACC, \"w\") as fout_acc, \\\n",
    "            open(parsed_dir + OUTPUT_Ans, \"w\") as fout_ans:\n",
    "        for line in fin:\n",
    "            data = json.loads(line)\n",
    "            qid = data['QuestionId']\n",
    "            aid_list = data['AnswererIdList']\n",
    "            accaid = data['AcceptedAnswererId']\n",
    "            ans_list = data['AnswererAnswerTuples']\n",
    "            accsid = data['AcceptedAnswerId']\n",
    "            #for aid in aid_list:\n",
    "            #    part_user.add(int(aid))\n",
    "            #    print(\"{} {}\".format(str(qid), str(aid), str()), file=fout_a)\n",
    "            for ansid in ans_list:\n",
    "                print(\"{} {} {}\".format(str(qid), str(ansid[1]), str(ansid[0])), file=fout_ans)\n",
    "                part_user.add(int(ansid[0]))\n",
    "                print(\"{} {} {} \".format(str(qid),str(ansid[0]), str(ansid[2])), file=fout_a)\n",
    "            print(\"{} {}\".format(str(qid), str(accaid)), file=fout_acc)\n",
    "\n",
    "\n",
    "def extract_question_content(data_dir, parsed_dir):\n",
    "    \"\"\"Extract questions, content pairs from question file\n",
    "\n",
    "    Question content pair format:\n",
    "        <qid> <content>\n",
    "    We extract both with and without stop-word version\n",
    "        which is signified by \"_nsw\"\n",
    "\n",
    "    Args:\n",
    "        data_dir - data directory\n",
    "        parsed_dir - parsed file directory\n",
    "    \"\"\"\n",
    "    INPUT = \"Posts_Q.json\"\n",
    "    OUTPUT_T = \"Q_title.txt\"  # Question title\n",
    "    OUTPUT_T_NSW = \"Q_title_nsw.txt\"  # Question title, no stop word\n",
    "    OUTPUT_C = \"Q_content.txt\"  # Question content\n",
    "    OUTPUT_C_NSW = \"Q_content_nsw.txt\"  # Question content, no stop word\n",
    "    \n",
    "    \n",
    "    INPUT_A = \"Posts_A.json\"\n",
    "    OUTPUT_A = \"A_content.txt\"  # ANSWER \n",
    "    OUTPUT_A_NSW = \"A_content_nsw.txt\"  # ANSWER , no stop word\n",
    "   \n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT):\n",
    "        IOError(\"Can NOT locate {}\".format(data_dir + INPUT))\n",
    "\n",
    "    sw_set = set(stopwords.words('english'))  # Create the stop word set\n",
    "\n",
    "    # We will try both with or without stopwords to\n",
    "    # check out the performance.\n",
    "    with open(data_dir + INPUT, \"r\") as fin, \\\n",
    "            open(parsed_dir + OUTPUT_T, \"w\") as fout_t, \\\n",
    "            open(parsed_dir + OUTPUT_T_NSW, \"w\") as fout_t_nsw, \\\n",
    "            open(parsed_dir + OUTPUT_C, \"w\") as fout_c, \\\n",
    "            open(parsed_dir + OUTPUT_C_NSW, \"w\") as fout_c_nsw:\n",
    "        \n",
    "        for line in fin:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                qid = data.get('Id')\n",
    "                if qid not in qa_map:\n",
    "                    continue\n",
    "                title = data.get('Title')\n",
    "                content = data.get('Body')\n",
    "                \n",
    "                content, title = clean_str2(content)[:255], clean_str2(title)[:255]\n",
    "                \n",
    "                content_nsw = content\n",
    "                #content_nsw = remove_stopwords(content, sw_set)\n",
    "                \n",
    "                title_nsw = title\n",
    "                #title_nsw = remove_stopwords(title, sw_set)\n",
    "                \n",
    "                print(\"{} {}\".format(qid, content_nsw),\n",
    "                      file=fout_c_nsw)  # Without stopword\n",
    "                \n",
    "                print(\"{} {}\".format(qid, content),\n",
    "                      file=fout_c)  # With stopword\n",
    "                print(\"{} {}\".format(qid, title_nsw),\n",
    "                      file=fout_t_nsw)  # Without stopword\n",
    "                print(\"{} {}\".format(qid, title),\n",
    "                      file=fout_t)  # With stopword\n",
    "            except:\n",
    "                print(\"e\")\n",
    "                #logger.info(\"Error at Extracting question content and title: \"\n",
    "                          #  + str(data))\n",
    "                continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    with open(data_dir + INPUT_A, \"r\") as fina, \\\n",
    "            open(parsed_dir + OUTPUT_A, \"w\") as foua_t, \\\n",
    "            open(parsed_dir + OUTPUT_A_NSW, \"w\") as foua_t_nsw:\n",
    "         \n",
    "        \n",
    "        for line in fina:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            ID = data.get('Id')\n",
    "            parentid = data.get('ParentId')\n",
    "            OwnerUserId = data.get('OwnerUserId')\n",
    "            Body = data.get('Body')\n",
    "            Score = data.get('Score')\n",
    "\n",
    "            Body = clean_str2(Body)[:255]\n",
    "\n",
    "            #print(OwnerUserId)\n",
    "            print(\"{} {} {} {} {}\".format(parentid, ID, OwnerUserId, Score  ,Body),\n",
    "                  file=foua_t_nsw)  # Without stopword\n",
    "\n",
    "            print(\"{} {} {} {} {}\".format(parentid, ID, OwnerUserId, Score  ,Body),\n",
    "                  file=foua_t)  # With stopword\n",
    "\n",
    "            \n",
    "\n",
    "def extract_answer_score(data_dir, parsed_dir):\n",
    "    \"\"\"Extract the answers vote, a.k.a. Scores.\n",
    "\n",
    "    This information might be useful when\n",
    "        the accepted answer is not selected.\n",
    "\n",
    "    Args:\n",
    "        data_dir - Input data dir\n",
    "        parsed_dir - Output data dir\n",
    "    \"\"\"\n",
    "    INPUT = \"Posts_A.json\"\n",
    "    OUTPUT = \"A_score.txt\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT):\n",
    "        IOError(\"Cannot find file{}\".format(data_dir + INPUT))\n",
    "\n",
    "    with open(data_dir + INPUT, \"r\") as fin, \\\n",
    "        open(parsed_dir + OUTPUT, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                aid = data.get('Id')\n",
    "                score = data.get('Score')\n",
    "                print(\"{} {}\".format(aid, score), file=fout)\n",
    "            except:\n",
    "                logging.info(\"Error at Extracting answer score: \"\n",
    "                             + str(data))\n",
    "                continue\n",
    "\n",
    "\n",
    "def extract_question_best_answerer(data_dir, parsed_dir):\n",
    "    \"\"\"Extract the question-best-answerer relation\n",
    "\n",
    "    Args:\n",
    "        data_dir  - as usual\n",
    "        parsed_dir  -  as usual\n",
    "    \"\"\"\n",
    "    INPUT_A = \"Posts_A.json\"\n",
    "    INPUT_MAP = \"Record_Train.json\"\n",
    "    OUTPUT = \"Q_ACC_A.txt\"\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT_A):\n",
    "        IOError(\"Cannot find file {}\".format(data_dir + INPUT_A))\n",
    "    if not os.path.exists(data_dir + INPUT_MAP):\n",
    "        IOError(\"Cannot find file {}\".format(data_dir + INPUT_MAP))\n",
    "\n",
    "    accanswerid_uaid = {}  # Accepted answer id to Answering user id\n",
    "    answerid_score = {}  # Answer id to answer scores\n",
    "    with open(data_dir + INPUT_A, \"r\") as fin_a, \\\n",
    "        open(data_dir + INPUT_MAP, \"r\") as fin_map, \\\n",
    "        open(parsed_dir + OUTPUT, \"w\") as fout:\n",
    "\n",
    "        # build acc-a dict\n",
    "        for line in fin_a:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                answerid = data.get(\"Id\")\n",
    "                if answerid == \"5\":\n",
    "                    print(100)\n",
    "                score = data.get(\"Score\")\n",
    "                uaid = data.get(\"OwnerUserId\")\n",
    "                answerid_score[answerid] = score\n",
    "                accanswerid_uaid[answerid] = uaid  # uaid is rid\n",
    "            except:\n",
    "                logging.info(\n",
    "                    \"Error at Extracting question, best answer user: \"\n",
    "                    + str(data))\n",
    "\n",
    "        print(len(accanswerid_uaid))\n",
    "        for line in fin_map:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                qid = data.get('QuestionId')\n",
    "                if \"AcceptedAnswerID\" in data:  # If acc answer exists\n",
    "                    acc_answerid = data.get('AcceptedAnswerId')\n",
    "                else:\n",
    "                # If acc answer doesn't exist, choose highest score answer\n",
    "                    ans = data.get('AnswerIdList')\n",
    "                    ans = list(zip(*ans))[0]\n",
    "                    scores = [answerid_score[answerid] for answerid in ans]\n",
    "                    max_ind = scores.index(max(scores))\n",
    "                    acc_answerid = ans[max_ind]\n",
    "                uaccid = accanswerid_uaid[acc_aid]\n",
    "                print(\"{} {}\".format(qid, uaccid), file=fout)\n",
    "            except:\n",
    "                print(1)\n",
    "                logging.info(\n",
    "                    \"Error at Extracting question, best answer user: \"\n",
    "                     + str(data))\n",
    "\n",
    "\n",
    "def extract_question_best_answerer_2(data_dir, parsed_dir):\n",
    "    \"\"\"Extract the question-best-answerer relation\n",
    "\n",
    "    Args:\n",
    "        data_dir  - as usual\n",
    "        parsed_dir  -  as usual\n",
    "    \"\"\"\n",
    "    INPUT_A = \"Posts_A.json\"\n",
    "    # INPUT_MAP = \"Record_Train.json\"\n",
    "    # Uncomment this when running NeRank\n",
    "    INPUT_MAP = \"Record_All.json\"\n",
    "    OUTPUT = \"Q_ACC_A.txt\"\n",
    "\n",
    "    if not os.path.exists(data_dir + INPUT_A):\n",
    "        IOError(\"Cannot find file {}\".format(data_dir + INPUT_A))\n",
    "    if not os.path.exists(data_dir + INPUT_MAP):\n",
    "        IOError(\"Cannot find file {}\".format(data_dir + INPUT_MAP))\n",
    "\n",
    "    accanswerid_uaid = {}  # Accepted answer id to Answering user id\n",
    "    answerid_score = {}  # Answer id to answer scores\n",
    "    with open(data_dir + INPUT_MAP, \"r\") as fin_map, \\\n",
    "        open(parsed_dir + OUTPUT, \"w\") as fout:\n",
    "\n",
    "        for line in fin_map:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                qid = data.get('QuestionId')\n",
    "                acc_aid = data.get(\"AcceptedAnswererId\")\n",
    "                if qid and acc_aid:\n",
    "                    print(\"{} {}\".format(qid, acc_aid), file=fout)\n",
    "            except:\n",
    "                \n",
    "                logging.info(\n",
    "                    \"Error at Extracting question, best answer user: \"\n",
    "                     + str(data))\n",
    "\n",
    "def write_part_users(parsed_dir):\n",
    "    OUTPUT = \"QA_ID.txt\"\n",
    "    with open(parsed_dir + OUTPUT, \"w\") as fout:\n",
    "        IdList = list(part_user)\n",
    "        IdList.sort()\n",
    "        for index, user_id in enumerate(IdList):\n",
    "            print(\"{} {}\".format(index + 1, user_id), file=fout)\n",
    "\n",
    "\n",
    "def preprocess_(dataset, threshold, prop_test, sample_size):\n",
    "    DATASET = dataset\n",
    "    RAW_DIR = os.getcwd() + \"/raw/{}/\".format(DATASET)\n",
    "    DATA_DIR= os.getcwd() + \"/data/{}/\".format(DATASET)\n",
    "    PARSED_DIR = os.getcwd() + \"/data/parsed/{}/\".format(DATASET)\n",
    "\n",
    "    print(\"Preprocessing {} ...\".format(dataset))\n",
    "\n",
    "    if not os.path.exists(RAW_DIR):\n",
    "        print(\"{} dir or path doesn't exist.\\n\"\n",
    "              \"Please download the raw data set into the /raw.\"\n",
    "              .format(RAW_DIR), file=sys.stderr)\n",
    "        sys.exit()\n",
    "\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(\"{} data dir not found.\\n\"\n",
    "              \" Creating a folder for that.\"\n",
    "              .format(DATA_DIR))\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(PARSED_DIR):\n",
    "        print(\"{} dir or path NOT found.\\n\"\n",
    "              \"Creating a folder for that.\"\n",
    "              .format(PARSED_DIR))\n",
    "        os.makedirs(PARSED_DIR)\n",
    "\n",
    "    if os.path.exists(DATA_DIR + \"log.log\"):\n",
    "        os.remove(DATA_DIR + \"log.log\")\n",
    "\n",
    "    # Setting up loggers\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    log_fh = logging.FileHandler(DATA_DIR + \"log.log\")\n",
    "    log_fh.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    log_fh.setFormatter(formatter)\n",
    "    logger.addHandler(log_fh)\n",
    "\n",
    "    # Split contest to question and answer\n",
    "    print(\"\\tSpliting post\")\n",
    "    split_post(raw_dir=RAW_DIR, data_dir=DATA_DIR)\n",
    "\n",
    "    # Extract question-user, answer-user, and question-answer information\n",
    "    # Generate Question and Answer/User map\n",
    "    print(\"\\tProcessing QA\")\n",
    "    process_QA(data_dir=DATA_DIR)\n",
    "\n",
    "    print(\"\\tGenerating question statistics...\")\n",
    "    question_stats(data_dir=DATA_DIR)\n",
    "\n",
    "    print(\"\\tExtracting question content ...\")\n",
    "    extract_question_content(data_dir=DATA_DIR, parsed_dir=PARSED_DIR)\n",
    "\n",
    "    print(\"\\tBuilding test sets\")\n",
    "    build_test_set(data_dir=DATA_DIR, parsed_dir=PARSED_DIR,\n",
    "                   threshold=threshold, test_sample_size=sample_size,\n",
    "                   test_proportion=prop_test)\n",
    "\n",
    "    print(\"\\tExtracting Q, R, A relations ...\")\n",
    "    extract_question_user(data_dir=DATA_DIR, parsed_dir=PARSED_DIR)\n",
    "\n",
    "    extract_question_answer_user(data_dir=DATA_DIR, parsed_dir=PARSED_DIR)\n",
    "\n",
    "\n",
    "    extract_answer_score(data_dir=DATA_DIR, parsed_dir=PARSED_DIR)\n",
    "    extract_question_best_answerer_2(data_dir=DATA_DIR, parsed_dir=PARSED_DIR)\n",
    "\n",
    "    write_part_users(parsed_dir=PARSED_DIR)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Biology ...\n",
      "F:\\服务器914\\开源\\MPQR/data/Biology/ data dir not found.\n",
      " Creating a folder for that.\n",
      "F:\\服务器914\\开源\\MPQR/data/parsed/Biology/ dir or path NOT found.\n",
      "Creating a folder for that.\n",
      "\tSpliting post\n",
      "\tProcessing QA\n",
      "\t\t12942 questions do not have accepted answer!\n",
      "\t\tWriting the Record for ALL to disk.\n",
      "\tGenerating question statistics...\n",
      "0 answer id list 251\n",
      "0 answer id list 524\n",
      "0 answer id list 640\n",
      "0 answer id list 1214\n",
      "0 answer id list 1244\n",
      "0 answer id list 1247\n",
      "0 answer id list 1326\n",
      "0 answer id list 1383\n",
      "0 answer id list 1655\n",
      "0 answer id list 1690\n",
      "0 answer id list 1758\n",
      "0 answer id list 1975\n",
      "0 answer id list 2028\n",
      "0 answer id list 2112\n",
      "0 answer id list 2124\n",
      "0 answer id list 2272\n",
      "0 answer id list 3515\n",
      "0 answer id list 3839\n",
      "0 answer id list 4946\n",
      "0 answer id list 5034\n",
      "0 answer id list 5063\n",
      "0 answer id list 5222\n",
      "0 answer id list 5380\n",
      "0 answer id list 5554\n",
      "0 answer id list 5576\n",
      "0 answer id list 5578\n",
      "0 answer id list 6790\n",
      "0 answer id list 7710\n",
      "0 answer id list 8837\n",
      "0 answer id list 8890\n",
      "0 answer id list 9225\n",
      "0 answer id list 9312\n",
      "0 answer id list 9332\n",
      "0 answer id list 9345\n",
      "0 answer id list 9365\n",
      "0 answer id list 9375\n",
      "0 answer id list 9387\n",
      "0 answer id list 9388\n",
      "0 answer id list 9524\n",
      "0 answer id list 10069\n",
      "0 answer id list 10764\n",
      "0 answer id list 11270\n",
      "0 answer id list 11392\n",
      "0 answer id list 13536\n",
      "0 answer id list 13879\n",
      "0 answer id list 14242\n",
      "0 answer id list 15605\n",
      "0 answer id list 16986\n",
      "0 answer id list 17189\n",
      "0 answer id list 17522\n",
      "0 answer id list 19974\n",
      "0 answer id list 25938\n",
      "0 answer id list 30649\n",
      "0 answer id list 34765\n",
      "0 answer id list 35887\n",
      "0 answer id list 42792\n",
      "0 answer id list 43637\n",
      "0 answer id list 45683\n",
      "0 answer id list 46744\n",
      "0 answer id list 47814\n",
      "0 answer id list 47828\n",
      "0 answer id list 47928\n",
      "0 answer id list 48463\n",
      "0 answer id list 48676\n",
      "0 answer id list 48812\n",
      "0 answer id list 48896\n",
      "0 answer id list 48982\n",
      "0 answer id list 50807\n",
      "0 answer id list 51742\n",
      "0 answer id list 54429\n",
      "0 answer id list 54606\n",
      "0 answer id list 54725\n",
      "0 answer id list 54762\n",
      "0 answer id list 55088\n",
      "0 answer id list 56943\n",
      "0 answer id list 58916\n",
      "0 answer id list 59188\n",
      "0 answer id list 59248\n",
      "0 answer id list 59354\n",
      "0 answer id list 59391\n",
      "0 answer id list 59404\n",
      "0 answer id list 59534\n",
      "0 answer id list 59541\n",
      "0 answer id list 59648\n",
      "0 answer id list 59695\n",
      "0 answer id list 59814\n",
      "0 answer id list 59973\n",
      "0 answer id list 60120\n",
      "0 answer id list 60164\n",
      "0 answer id list 60246\n",
      "0 answer id list 60274\n",
      "0 answer id list 60288\n",
      "0 answer id list 60363\n",
      "0 answer id list 60398\n",
      "0 answer id list 60428\n",
      "0 answer id list 61091\n",
      "0 answer id list 61198\n",
      "0 answer id list 61300\n",
      "0 answer id list 61470\n",
      "0 answer id list 62714\n",
      "0 answer id list 62720\n",
      "0 answer id list 62726\n",
      "0 answer id list 63947\n",
      "0 answer id list 63969\n",
      "0 answer id list 63994\n",
      "0 answer id list 64026\n",
      "0 answer id list 64069\n",
      "0 answer id list 64072\n",
      "0 answer id list 64192\n",
      "0 answer id list 64268\n",
      "0 answer id list 64341\n",
      "0 answer id list 64364\n",
      "0 answer id list 64434\n",
      "0 answer id list 64472\n",
      "0 answer id list 64556\n",
      "0 answer id list 64616\n",
      "0 answer id list 64627\n",
      "0 answer id list 64640\n",
      "0 answer id list 64670\n",
      "0 answer id list 64671\n",
      "0 answer id list 64684\n",
      "0 answer id list 64713\n",
      "0 answer id list 64764\n",
      "0 answer id list 64777\n",
      "0 answer id list 64833\n",
      "0 answer id list 64860\n",
      "0 answer id list 64869\n",
      "0 answer id list 64887\n",
      "0 answer id list 64936\n",
      "0 answer id list 65027\n",
      "0 answer id list 65048\n",
      "0 answer id list 65089\n",
      "0 answer id list 65104\n",
      "0 answer id list 65180\n",
      "0 answer id list 65399\n",
      "0 answer id list 65449\n",
      "0 answer id list 65466\n",
      "0 answer id list 65490\n",
      "0 answer id list 65523\n",
      "0 answer id list 65577\n",
      "0 answer id list 65672\n",
      "0 answer id list 65686\n",
      "0 answer id list 65742\n",
      "0 answer id list 65855\n",
      "0 answer id list 66000\n",
      "0 answer id list 66042\n",
      "0 answer id list 66139\n",
      "0 answer id list 66188\n",
      "0 answer id list 66270\n",
      "0 answer id list 66277\n",
      "0 answer id list 66309\n",
      "0 answer id list 66321\n",
      "0 answer id list 66449\n",
      "0 answer id list 66507\n",
      "0 answer id list 66566\n",
      "0 answer id list 66705\n",
      "0 answer id list 66729\n",
      "0 answer id list 66766\n",
      "0 answer id list 66898\n",
      "0 answer id list 66934\n",
      "0 answer id list 67064\n",
      "0 answer id list 67184\n",
      "0 answer id list 67641\n",
      "0 answer id list 67681\n",
      "0 answer id list 67795\n",
      "0 answer id list 67819\n",
      "0 answer id list 67867\n",
      "0 answer id list 67906\n",
      "0 answer id list 67943\n",
      "0 answer id list 67970\n",
      "0 answer id list 68132\n",
      "0 answer id list 68242\n",
      "0 answer id list 68269\n",
      "0 answer id list 68276\n",
      "0 answer id list 68358\n",
      "0 answer id list 68361\n",
      "0 answer id list 68371\n",
      "0 answer id list 68459\n",
      "0 answer id list 68504\n",
      "0 answer id list 68714\n",
      "0 answer id list 68745\n",
      "0 answer id list 68815\n",
      "0 answer id list 69236\n",
      "0 answer id list 69357\n",
      "0 answer id list 69524\n",
      "0 answer id list 69684\n",
      "0 answer id list 69725\n",
      "0 answer id list 70036\n",
      "0 answer id list 73600\n",
      "0 answer id list 77038\n",
      "0 answer id list 77144\n",
      "0 answer id list 77362\n",
      "0 answer id list 77506\n",
      "0 answer id list 84585\n",
      "\tExtracting question content ...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'F:\\\\ANACONDA\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\share\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/workspace/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'F:\\\\ANACONDA\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\share\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/workspace/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ad6095d1cc37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_proportion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msample_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpreprocess_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Biology'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-dddbcd3ac55a>\u001b[0m in \u001b[0;36mpreprocess_\u001b[1;34m(dataset, threshold, prop_test, sample_size)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\tExtracting question content ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m     \u001b[0mextract_question_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPARSED_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\tBuilding test sets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-dddbcd3ac55a>\u001b[0m in \u001b[0;36mextract_question_content\u001b[1;34m(data_dir, parsed_dir)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can NOT locate {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mINPUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[0msw_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Create the stop word set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;31m# We will try both with or without stopwords to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ANACONDA\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'F:\\\\ANACONDA\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\share\\\\nltk_data'\n    - 'F:\\\\ANACONDA\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/workspace/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #if len(sys.argv) < 3 + 1:\n",
    "     #   print(\"\\t Usage: {} [name of dataset] [threshold] [prop of test] [test sample size]\"\n",
    "     #         .format(sys.argv[0]), file=sys.stderr)\n",
    "     #   sys.exit(0)\n",
    "    threshold = int(5)\n",
    "    test_proportion = float(0.1)\n",
    "    sample_size = int(20)\n",
    "    preprocess_('Biology', threshold, test_proportion, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
